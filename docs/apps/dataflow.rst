DataFlow - Database Framework for Workflow Automation
========================================================

.. image:: https://img.shields.io/badge/app-dataflow-blue.svg
   :alt: DataFlow Application

.. image:: https://img.shields.io/badge/workflow--database-integration-green.svg
   :alt: Workflow Database Integration

DataFlow is a database framework built on the Kailash SDK that automatically generates workflow nodes from model definitions, providing seamless database integration for workflow automation systems.

Overview
--------

DataFlow transforms Python model definitions into complete database workflows with automatic schema management, CRUD operations, and enterprise-grade features. Each model automatically generates 9 workflow nodes for comprehensive database operations.

Key Features
------------

ðŸ”§ **Model-to-Node Generation**
   Define a model with ``@db.model`` and get 9 workflow nodes automatically: Create, Read, Update, Delete, List, and bulk operations.

ðŸ—„ï¸ **Multi-Database Support**
   Full PostgreSQL support with SQLite compatibility. Automatic schema migrations and connection pooling.

âš¡ **Workflow Integration**
   Generated nodes integrate seamlessly with Kailash workflows using LocalRuntime and WorkflowBuilder.

ðŸš€ **Enterprise Features**
   Connection pooling, transaction management, schema state management, and concurrent access protection.

ðŸ¢ **Production Ready**
   Real database operations with SQL injection protection, connection pooling for 10,000+ concurrent connections.

Installation
------------

.. code-block:: bash

   # Install DataFlow directly
   pip install kailash-dataflow

   # Or as part of Kailash
   pip install kailash[dataflow]

Quick Start
-----------

**Model Definition and Node Generation:**

.. code-block:: python

   from dataflow import DataFlow

   # Connect to database
   db = DataFlow("postgresql://user:pass@localhost/dbname")
   # For development: db = DataFlow("sqlite:///app.db")

   # Define your model
   @db.model
   class User:
       id: int        # Primary key (auto-generated)
       name: str      # Required field
       email: str     # Required field
       active: bool = True  # Optional with default

   # DataFlow automatically generates 9 workflow nodes:
   # UserCreateNode, UserReadNode, UserUpdateNode, UserDeleteNode,
   # UserListNode, UserBulkCreateNode, UserBulkUpdateNode,
   # UserBulkDeleteNode, UserBulkUpsertNode

**Using Generated Nodes in Workflows:**

.. code-block:: python

   from kailash.workflow.builder import WorkflowBuilder
   from kailash.runtime.local import LocalRuntime

   # Create workflow using generated nodes
   workflow = WorkflowBuilder()

   # Create a user
   workflow.add_node("UserCreateNode", "create_user", {
       "name": "John Doe",
       "email": "john@example.com"
   })

   # Read the user
   workflow.add_node("UserReadNode", "read_user", {
       "conditions": {"name": "John Doe"}
   })

   # Execute the workflow
   runtime = LocalRuntime()
   results, run_id = runtime.execute(workflow.build())

   print(f"Created user: {results['create_user']}")
   print(f"Found user: {results['read_user']}")

**Bulk Operations:**

.. code-block:: python

   # Bulk create multiple users
   workflow.add_node("UserBulkCreateNode", "bulk_create", {
       "data": [
           {"name": "Alice Smith", "email": "alice@example.com"},
           {"name": "Bob Jones", "email": "bob@example.com"},
           {"name": "Carol White", "email": "carol@example.com"}
       ]
   })

   # Bulk update with conditions
   workflow.add_node("UserBulkUpdateNode", "bulk_update", {
       "conditions": {"active": False},
       "data": {"active": True}
   })

   # Execute bulk operations
   results, run_id = runtime.execute(workflow.build())
   print(f"Bulk created: {results['bulk_create']}")

**Query Builder Integration:**

.. code-block:: python

   # Use MongoDB-style query builder
   query = User.query_builder()
   query.filter({"age": {"$gt": 18}})
   query.filter({"department": {"$in": ["engineering", "sales"]}})
   query.sort({"created_at": -1})
   query.limit(10)

   # Convert to workflow node
   workflow.add_node("UserListNode", "filtered_users", query.to_params())

   results, run_id = runtime.execute(workflow.build())
   users = results["filtered_users"]

Advanced Features
-----------------

**Multi-Database Configuration:**

.. code-block:: python

   # Configure with connection string
   db = DataFlow("postgresql://user:pass@localhost/dbname",
                pool_size=20, max_overflow=10)

   # Or with explicit configuration
   from dataflow.core.config import DataFlowConfig

   config = DataFlowConfig(
       database_url="postgresql://user:pass@localhost/dbname",
       pool_size=20,
       max_overflow=10,
       auto_migrate=True,
       existing_schema_mode=False
   )
   db = DataFlow(config=config)

**Model Relationships:**

.. code-block:: python

   # Define related models
   @db.model
   class User:
       id: int
       name: str
       email: str

   @db.model
   class Post:
       id: int
       title: str
       content: str
       author_id: int  # Foreign key to User

   # Use in workflows with proper relationships
   workflow = WorkflowBuilder()
   workflow.add_node("UserCreateNode", "create_author", {
       "name": "John Doe", "email": "john@example.com"
   })
   workflow.add_node("PostCreateNode", "create_post", {
       "title": "My First Post",
       "content": "Hello World!",
       "author_id": 1  # References created user
   })

**Schema Management:**

.. code-block:: python

   # DataFlow handles schema automatically
   db = DataFlow("postgresql://...", auto_migrate=True)

   # Or manage schema manually
   db = DataFlow("postgresql://...", auto_migrate=False, existing_schema_mode=True)

   # Get schema information
   models = db.list_models()
   model_info = db.get_model_info("User")
   generated_nodes = db.get_generated_nodes("User")

MongoDB-Style Query Builder
---------------------------

DataFlow provides MongoDB-style query builder that generates optimized SQL:

**Comparison Operators:**

.. code-block:: python

   # Build complex queries with MongoDB-style operators
   query = User.query_builder()
   query.filter({
       "age": {"$gt": 18, "$lt": 65},           # age > 18 AND age < 65
       "name": {"$regex": "^John"},             # name LIKE 'John%'
       "department": {"$in": ["eng", "sales"]}, # department IN ('eng', 'sales')
       "status": {"$ne": "inactive"}            # status != 'inactive'
   })

   # Use in workflow
   workflow.add_node("UserListNode", "filtered_users", query.to_params())

**Array and JSON Operations:**

.. code-block:: python

   # JSON/JSONB operations (PostgreSQL, MySQL 5.7+)
   profiles = app.query("users").where({
       "metadata.preferences.theme": "dark",   # JSON path queries
       "tags": {"$contains": "premium"},       # Array contains
       "skills": {"$size": {"$gt": 3}}         # Array size > 3
   })

**Aggregation Pipeline:**

.. code-block:: python

   # Complex aggregations with automatic SQL optimization
   results = app.query("orders").aggregate([
       {"$match": {"status": "completed"}},
       {"$group": {
           "_id": "$customer_id",
           "total_spent": {"$sum": "$amount"},
           "order_count": {"$sum": 1}
       }},
       {"$sort": {"total_spent": -1}},
       {"$limit": 100}
   ])

Production Examples
-------------------

**1. E-commerce Analytics Dashboard:**

.. code-block:: python

   from dataflow import DataFlow

   app = DataFlow()

   # Real-time sales metrics
   @app.endpoint("/metrics/sales")
   def sales_metrics():
       return {
           "today": app.query("orders").where({
               "created_at": {"$gte": {"$date": "today"}}
           }).aggregate([{"$sum": "$amount"}]),

           "top_products": app.query("order_items").aggregate([
               {"$group": {"_id": "$product_id", "sold": {"$sum": "$quantity"}}},
               {"$sort": {"sold": -1}},
               {"$limit": 10}
           ])
       }

   app.start()

**2. User Management API:**

.. code-block:: python

   # Multi-tenant user management
   @app.middleware("tenant_isolation")
   def add_tenant_filter(query, request):
       query.where({"tenant_id": request.headers.get("X-Tenant-ID")})

   # Automatic CRUD with security
   @app.resource("users")
   class UserResource:
       def before_create(self, data):
           data["created_at"] = datetime.utcnow()
           return data

       def before_update(self, data):
           data["updated_at"] = datetime.utcnow()
           return data

**3. Real-time Monitoring:**

.. code-block:: python

   # Performance monitoring with caching
   @app.cached(ttl=60)  # Cache for 1 minute
   def system_health():
       return {
           "active_users": app.query("sessions").where({
               "last_seen": {"$gte": {"$date": "-5m"}}
           }).count(),

           "error_rate": app.query("logs").where({
               "level": "ERROR",
               "timestamp": {"$gte": {"$date": "-1h"}}
           }).count()
       }

Performance & Optimization
--------------------------

**Query Performance:**
   - **31.8M operations/second** baseline performance
   - **Automatic query optimization** with SQL generation
   - **Connection pooling** with 10,000+ concurrent connections
   - **Prepared statement caching** for repeated queries

**Cache Performance:**
   - **99.9% hit rate** with intelligent invalidation
   - **Redis clustering** support for high availability
   - **Pattern-based invalidation** for complex relationships
   - **Automatic cache warming** for critical queries

**Benchmarks:**

.. code-block:: python

   # Built-in performance monitoring
   with app.benchmark("complex_query"):
       results = app.query("large_table").where({...}).aggregate([...])

   # View performance metrics
   print(app.metrics.summary())
   # Query: complex_query - Avg: 45ms, P95: 120ms, Count: 1,247

Enterprise Features
-------------------

**Multi-Tenant Architecture:**

.. code-block:: python

   # Automatic tenant isolation
   app = DataFlow({
       "multi_tenant": True,
       "tenant_header": "X-Tenant-ID"
   })

   # All queries automatically scoped to tenant
   users = app.query("users").where({"active": True})
   # Becomes: SELECT * FROM users WHERE tenant_id = ? AND active = true

**Security & Compliance:**

.. code-block:: python

   # GDPR/CCPA compliance built-in
   @app.compliance("gdpr")
   def handle_data_request(user_id, request_type):
       if request_type == "export":
           return app.export_user_data(user_id)
       elif request_type == "delete":
           return app.anonymize_user_data(user_id)

**Audit Logging:**

.. code-block:: python

   # Comprehensive audit trails
   app.enable_audit_log()

   # All data changes automatically logged
   app.update("users", {"id": 123}, {"status": "inactive"})
   # Audit log: {"user": "admin", "action": "update", "table": "users", ...}

Deployment
----------

**Docker Deployment:**

.. code-block:: dockerfile

   FROM python:3.11-slim
   COPY requirements.txt .
   RUN pip install kailash-dataflow
   COPY app.py .
   EXPOSE 8000
   CMD ["python", "app.py"]

**Kubernetes:**

.. code-block:: yaml

   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: dataflow-app
   spec:
     replicas: 3
     selector:
       matchLabels:
         app: dataflow
     template:
       metadata:
         labels:
           app: dataflow
       spec:
         containers:
         - name: dataflow
           image: my-dataflow-app:latest
           ports:
           - containerPort: 8000
           env:
           - name: DATABASE_URL
             valueFrom:
               secretKeyRef:
                 name: db-secret
                 key: url

**Environment Variables:**

.. code-block:: bash

   # Production configuration
   export DATAFLOW_DATABASE_URL="postgresql://..."
   export DATAFLOW_REDIS_URL="redis://..."
   export DATAFLOW_LOG_LEVEL="INFO"
   export DATAFLOW_ENABLE_METRICS="true"

Migration Guide
---------------

**From Raw SQL:**

.. code-block:: python

   # Before: Raw SQL
   cursor.execute("""
       SELECT department, COUNT(*) as count
       FROM users
       WHERE age > %s
       GROUP BY department
       ORDER BY count DESC
   """, (18,))

   # After: DataFlow
   results = app.query("users").where({"age": {"$gt": 18}}).aggregate([
       {"$group": {"_id": "$department", "count": {"$sum": 1}}},
       {"$sort": {"count": -1}}
   ])

**From ORM:**

.. code-block:: python

   # Before: SQLAlchemy ORM
   users = session.query(User).filter(
       User.age > 18,
       User.department.in_(['eng', 'sales'])
   ).order_by(User.created_at.desc()).limit(10).all()

   # After: DataFlow
   users = app.query("users").where({
       "age": {"$gt": 18},
       "department": {"$in": ["eng", "sales"]}
   }).sort({"created_at": -1}).limit(10)

API Reference
-------------

See the complete :doc:`DataFlow API Reference <../api/dataflow>` for detailed documentation of all classes and methods.

Examples Repository
-------------------

Complete production examples available in the ``apps/kailash-dataflow/examples/`` directory:

- **E-commerce Platform**: Complete online store with inventory, orders, and analytics
- **User Management System**: Multi-tenant user management with RBAC
- **Analytics Dashboard**: Real-time metrics with caching and aggregations
- **API Gateway**: High-performance API proxy with rate limiting

Support & Community
-------------------

- **GitHub**: `github.com/integrum/kailash-python-sdk <https://github.com/integrum/kailash-python-sdk>`_
- **Issues**: Report DataFlow-specific issues
- **Documentation**: Complete guides and tutorials
- **Examples**: Production-ready code samples
